{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Create a tiny language model from scratch, with only two attentional layers. This model is like a BERT model, but in very simplified version. The goal here is to understand what happens behind the scenes of self-attention based language models."
      ],
      "metadata": {
        "id": "RDKxLnVKjde3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Installations for TPU usages in Google Colab**"
      ],
      "metadata": {
        "id": "dF0Cj7GpkIee"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Runtime > Change runtime type > TPU\n",
        "!pip install -q -U accelerate\n",
        "!pip install -q -U cloud-tpu-client torch torchvision https://storage.googleapis.com/tpu-pytorch/wheels/colab/torch_xla-2.0-cp310-cp310-linux_x86_64.whl"
      ],
      "metadata": {
        "id": "lYTpgJ1MV_qj"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Imports**"
      ],
      "metadata": {
        "id": "L6e8Z4eNkRtM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch_xla\n",
        "import torch_xla.core.xla_model as xm"
      ],
      "metadata": {
        "id": "AAXwjhspkFUd"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create class**\n",
        "- PositionalEncoding - For positional encoding\n",
        "- MultiHeadSelfAttention - For self-attention layers\n",
        "- TinyLM - The full model flow"
      ],
      "metadata": {
        "id": "lXKNgEDrkWax"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "f0Hepf-_V3QA"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.pe = torch.randn(1, max_len, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:, :x.size(1), :]\n",
        "\n",
        "class MultiHeadSelfAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadSelfAttention, self).__init__()\n",
        "        assert d_model % num_heads == 0\n",
        "        self.d_head = d_model // num_heads\n",
        "        self.num_heads = num_heads\n",
        "        self.query = nn.Linear(d_model, d_model)\n",
        "        self.key = nn.Linear(d_model, d_model)\n",
        "        self.value = nn.Linear(d_model, d_model)\n",
        "        self.fc_out = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size = x.size(0)\n",
        "        Q = self.query(x)\n",
        "        K = self.key(x)\n",
        "        V = self.value(x)\n",
        "        Q = Q.view(batch_size, -1, self.num_heads, self.d_head).permute(0, 2, 1, 3)\n",
        "        K = K.view(batch_size, -1, self.num_heads, self.d_head).permute(0, 2, 1, 3)\n",
        "        V = V.view(batch_size, -1, self.num_heads, self.d_head).permute(0, 2, 1, 3)\n",
        "        attn_score = torch.matmul(Q, K.permute(0, 1, 3, 2)) / self.d_head**0.5\n",
        "        attn_prob = F.softmax(attn_score, dim=-1)\n",
        "        attn_output = torch.matmul(attn_prob, V).permute(0, 2, 1, 3).contiguous().view(batch_size, -1, self.num_heads * self.d_head)\n",
        "        return self.fc_out(attn_output)\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, d_model, d_ff):\n",
        "        super(FeedForward, self).__init__()\n",
        "        self.fc1 = nn.Linear(d_model, d_ff)\n",
        "        self.fc2 = nn.Linear(d_ff, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc2(F.relu(self.fc1(x)))\n",
        "\n",
        "class TinyLM(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model=16, num_heads=3, d_ff=64, max_len=512):\n",
        "        super(TinyLM, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.position_enc = PositionalEncoding(d_model, max_len)\n",
        "        self.attn1 = MultiHeadSelfAttention(d_model, num_heads)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.attn2 = MultiHeadSelfAttention(d_model, num_heads)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.feed_forward = FeedForward(d_model, d_ff)\n",
        "        self.norm3 = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, tokens):\n",
        "        embeddings = self.embedding(tokens)\n",
        "        x = embeddings + self.position_enc(embeddings)\n",
        "        attn1_out = self.norm1(x + self.attn1(x))\n",
        "        attn2_out = self.norm2(attn1_out + self.attn2(attn1_out))\n",
        "        ff_out = self.norm3(attn2_out + self.feed_forward(attn2_out))\n",
        "        return ff_out"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Example usage**"
      ],
      "metadata": {
        "id": "R-lPXdI3k0Az"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = xm.xla_device()\n",
        "\n",
        "tokens = torch.randint(0, 10000, (32, 100)).to(device)  # batch=32 sentences with 100 tokens each\n",
        "model = TinyLM(vocab_size=10000, d_model=16, num_heads=2).to(device)\n",
        "\n",
        "output = model(tokens)\n",
        "output = output.view(-1, output.size(-1))\n",
        "\n",
        "print(output.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cx8oZivBkyQV",
        "outputId": "4d3eb44e-81c9-40a5-b3c3-f5a9de85d046"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3200, 16])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kjQMgDqyZdGR"
      },
      "execution_count": 4,
      "outputs": []
    }
  ]
}